{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import option_context\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the main notebook for topic modeling and sentiment analysis (version 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99296, 24)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df[['username','tweet']].copy()\n",
    "df_analysis.dropna(inplace=True)\n",
    "df_analysis.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop.extend(['hi', 'hey', 'hello','ha', 'followed','wa','dm','dont','cant','wont','get','still','like','need',\n",
    "            'someone','people','im','ive','month','week','day','could','give','want','please','pls','since','one',\n",
    "             'back','thanks','thank','take','doesnt','does', 'might', 'must','lock','access','sent','closed','use',\n",
    "            'u','contact','new','email','time','message','tried','answer','waiting','issue','reason','going'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extend for most common English adverbs\n",
    "\n",
    "stop.extend(['up','so','out','just','now','how','then','more','also','here',\n",
    "            'well','only','very','even','back','there','down','still','in',\n",
    "            'as','to','when','never','really','most','on','why','about','over',\n",
    "            'again','where','right','off','always','today','all','far','long',\n",
    "            'away','yet','often','ever','however','almost','later','much',\n",
    "            'once','least','ago','together','around','already','enough','both',\n",
    "            'maybe','actually','probably','home','of course','perhaps','little',\n",
    "            'else','sometimes','finally','less','better','early','especially',\n",
    "            'either','quite','simply','nearly','soon','certainly','quickly',\n",
    "            'no','recently','before','usually','thus','exactly','hard',\n",
    "            'particularly','pretty','forward','ok','okay','clearly','indeed',\n",
    "            'rather','that','tonight','close','suddenly','best','instead',\n",
    "            'ahead','fast','alone','eventually','directly'])\n",
    "\n",
    "# # Extend for most common irregular verbs (except pay,lose,send,buy,spend)\n",
    "\n",
    "stop.extend(['say','make','go','take','come','see','know','get','got','give',\n",
    "            'find','think','tell','become','show','leave','feel','put','bring',\n",
    "            'begin','keep','hold','write','stand','hear','let','mean','set','meet',\n",
    "            'run','sit','speak','lie','lead','read','grow','fall',\n",
    "            'build','understand','draw','break','cut','rise','drive','wear',\n",
    "            'choose'])\n",
    "\n",
    "# ## Extend for prepositions\n",
    "stop.extend(['without','among'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_compounds = (lambda x: \n",
    "                 x.replace('debit card', 'debitcard')\n",
    "                 .replace('rainbow card', 'debitcard')\n",
    "                 .replace('bank card', 'debitcard')\n",
    "                 .replace('revolut card', 'debitcard')\n",
    "                 .replace('revcards', 'debitcard')\n",
    "                 .replace('credit card', 'creditcard')\n",
    "                 .replace('junior card', 'juniorcard')\n",
    "                 .replace('revolut junior', 'juniorcard')\n",
    "                 .replace('business account', 'budinessaccount')\n",
    "                 .replace('savings account', 'savingsaccount')\n",
    "                 .replace('bank account', 'bankaccount')\n",
    "                 .replace('premium account', 'premiumaccount')\n",
    "                 .replace('premium user', 'premiumaccount')\n",
    "                 .replace('premium plan', 'premiumaccount')\n",
    "                 .replace('metal account', 'metalaccount')\n",
    "                 .replace('metal card', 'metalaccount')\n",
    "                 .replace('metal customers', 'metalaccount')\n",
    "                 .replace('metal user', 'metalaccount')\n",
    "                 .replace('business bank', 'businessbank')\n",
    "                 .replace('virtual card', 'virtualcard')\n",
    "                 .replace('revolut business', 'revolutbusiness')\n",
    "                 .replace('google pay', 'googlepay')\n",
    "                 .replace('apple pay', 'applepay')\n",
    "                 .replace('applepayment', 'applepay')\n",
    "                 .replace('samsung pay', 'samsungpay')\n",
    "                 .replace('cryptocurrency', 'crypto')\n",
    "                 .replace('cryptos', 'crypto')\n",
    "                 .replace('cryptoasset', 'crypto')\n",
    "                 .replace('doge coin', 'crypto')\n",
    "                 .replace(' doge ', ' crypto ')\n",
    "                 .replace('customer service', 'customerservice')\n",
    "                 .replace('supoort', 'support')\n",
    "                 .replace('customer support', 'customerservice')\n",
    "                 .replace(' cs ', ' customerservice ')\n",
    "                 .replace('phone number', 'phonenumber')\n",
    "                 .replace('social media', 'socialmedia')\n",
    "                 .replace('app chat', 'appchat')\n",
    "                 .replace('challenger bank', 'fintech')\n",
    "                 .replace('challengerbank', 'fintech')\n",
    "                 .replace('neobanking', 'fintech')\n",
    "                 .replace('neobanks', 'fintech')\n",
    "                 .replace('neobank', 'fintech')\n",
    "                 .replace('transferthis', 'transfer')\n",
    "                 .replace('application', 'app')\n",
    "                 .replace('locked', 'lock')\n",
    "                 .replace('unlock', 'lock')\n",
    "                 .replace('block', 'lock')\n",
    "                 .replace('dark mode', 'darkmode')\n",
    "                 .replace('wealth management', 'wealthmanagement'))\n",
    "\n",
    "\n",
    "df_analysis['tweet_compound'] = df_analysis.tweet.map(create_compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for tweet in df_analysis['tweet_compound']:\n",
    "    tweet = lemma.lemmatize(tweet)\n",
    "    tweet = TextBlob(tweet).words  # tokenize words\n",
    "    tweet = [w for w in tweet if w not in stop]\n",
    "\n",
    "    counter += Counter(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32771"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             account 29068\n",
      "             revolut 17033\n",
      "                 app 16213\n",
      "               money 15544\n",
      "                lock 13818\n",
      "                help 12151\n",
      "                chat 7750\n",
      "                card 6317\n",
      "              access 6215\n",
      "             support 5496\n"
     ]
    }
   ],
   "source": [
    "for phrase, count in counter.most_common(10):\n",
    "    print('%20s %i' % (\"\".join(phrase), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tawneykirkland/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'course', 'doe', 'le', \"n't\", 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99296, 13152)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "    \n",
    "vectorizer = CountVectorizer(tokenizer = LemmaTokenizer(),\n",
    "                              stop_words = stop,\n",
    "                              min_df=2,\n",
    "                              max_df = 0.9)\n",
    "\n",
    "doc_word = vectorizer.fit_transform(df_analysis.tweet_compound)\n",
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99296, 5)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model = NMF(5)\n",
    "doc_topic = nmf_model.fit_transform(doc_word)\n",
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 13152)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = nmf_model.components_\n",
    "topic_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['account', 'fund', 'document', 'open', 'information', 'customerservice'],\n",
       " ['app', 'phone', 'card', 'support', 'log', 'work'],\n",
       " ['revolut', 'bank', 'crypto', 'customer', 'fintech', 'child'],\n",
       " ['money', 'bank', 'transfer', 'send', 'card', 'pay'],\n",
       " ['help', 'chat', 'agent', 'live', 'support', 'card']]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "t = nmf_model.components_.argsort(axis=1)[:,-1:-7:-1]\n",
    "topic_words = [[words[e] for e in l] for l in t]\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Component 0 (topic 1) seems to be about account queries\n",
    "- Component 1 (topic 2) seems to be about app-related queries\n",
    "- Component 2 (topic 3) seems to be about fintech innovations\n",
    "- Component 3 (topic 4) is about transfers and not being able to access accounts / money\n",
    "- Component 4 (topic 5) is about general requests for support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['topic'] = doc_topic.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_compound</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91573</th>\n",
       "      <td>stefanpaetow</td>\n",
       "      <td>well he left uber perhaps for a reason</td>\n",
       "      <td>well he left uber perhaps for a reason</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8444</th>\n",
       "      <td>stevenbarr321</td>\n",
       "      <td>cant deposit any money onto my card</td>\n",
       "      <td>cant deposit any money onto my card</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31543</th>\n",
       "      <td>katzreilly</td>\n",
       "      <td>ya the top up ita just floating around in cyber space</td>\n",
       "      <td>ya the top up ita just floating around in cyber space</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16044</th>\n",
       "      <td>heyimwalshy</td>\n",
       "      <td>thats perfect thank you</td>\n",
       "      <td>thats perfect thank you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96027</th>\n",
       "      <td>dsteingruberch</td>\n",
       "      <td>just backed a payments startup that powers popular fintech apps like  and  innovation fintech platforms via</td>\n",
       "      <td>just backed a payments startup that powers popular fintech apps like  and  innovation fintech platforms via</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             username  \\\n",
       "91573    stefanpaetow   \n",
       "8444    stevenbarr321   \n",
       "31543      katzreilly   \n",
       "16044     heyimwalshy   \n",
       "96027  dsteingruberch   \n",
       "\n",
       "                                                                                                                   tweet  \\\n",
       "91573                                                                             well he left uber perhaps for a reason   \n",
       "8444                                                                                 cant deposit any money onto my card   \n",
       "31543                                                             ya the top up ita just floating around in cyber space    \n",
       "16044                                                                                            thats perfect thank you   \n",
       "96027   just backed a payments startup that powers popular fintech apps like  and  innovation fintech platforms via        \n",
       "\n",
       "                                                                                                          tweet_compound  \\\n",
       "91573                                                                             well he left uber perhaps for a reason   \n",
       "8444                                                                                 cant deposit any money onto my card   \n",
       "31543                                                             ya the top up ita just floating around in cyber space    \n",
       "16044                                                                                            thats perfect thank you   \n",
       "96027   just backed a payments startup that powers popular fintech apps like  and  innovation fintech platforms via        \n",
       "\n",
       "       topic  \n",
       "91573      3  \n",
       "8444       3  \n",
       "31543      2  \n",
       "16044      2  \n",
       "96027      2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with option_context('display.max_colwidth', 600):\n",
    "    display(df_analysis.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "sentiment = []\n",
    "for tweet in df_analysis.tweet_compound:\n",
    "    sentiment.append(sid_obj.polarity_scores(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.329</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.138</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.165</td>\n",
       "      <td>-0.1531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound\n",
       "0  0.000  0.722  0.278    0.4019\n",
       "1  0.329  0.671  0.000   -0.9136\n",
       "2  0.138  0.862  0.000   -0.7269\n",
       "3  0.000  0.952  0.048    0.2023\n",
       "4  0.139  0.696  0.165   -0.1531"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df = pd.DataFrame(sentiment)\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([df_analysis, sentiment_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([df,merged_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - high level topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add month feature to enable analysis over time\n",
    "full_df['month'] = pd.DatetimeIndex(full_df['date']).month\n",
    "full_df['day'] = pd.DatetimeIndex(full_df['date']).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>...</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_compound</th>\n",
       "      <th>topic</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68188</th>\n",
       "      <td>68188</td>\n",
       "      <td>1259840548543827972</td>\n",
       "      <td>1259840548543827972</td>\n",
       "      <td>2020-05-11 09:39:28 EDT</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>09:39:28</td>\n",
       "      <td>-500</td>\n",
       "      <td>2817365839</td>\n",
       "      <td>jarokrolewski</td>\n",
       "      <td>really cool  for wallet management</td>\n",
       "      <td>...</td>\n",
       "      <td>jarokrolewski</td>\n",
       "      <td>really cool  for wallet management</td>\n",
       "      <td>really cool  for wallet management</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.3804</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92024</th>\n",
       "      <td>92024</td>\n",
       "      <td>1229375718183915520</td>\n",
       "      <td>1229375718183915520</td>\n",
       "      <td>2020-02-17 07:03:06 EST</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>07:03:06</td>\n",
       "      <td>-500</td>\n",
       "      <td>1412497622</td>\n",
       "      <td>clausematch</td>\n",
       "      <td>digital bank  has more than tripled its valuation to  bn   bn after closing the funding round heres how this fast growing fintech ensures compliance</td>\n",
       "      <td>...</td>\n",
       "      <td>clausematch</td>\n",
       "      <td>digital bank  has more than tripled its valuation to  bn   bn after closing the funding round heres how this fast growing fintech ensures compliance</td>\n",
       "      <td>digital bank  has more than tripled its valuation to  bn   bn after closing the funding round heres how this fast growing fintech ensures compliance</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                   id      conversation_id  \\\n",
       "68188       68188  1259840548543827972  1259840548543827972   \n",
       "92024       92024  1229375718183915520  1229375718183915520   \n",
       "\n",
       "                    created_at        date      time  timezone     user_id  \\\n",
       "68188  2020-05-11 09:39:28 EDT  2020-05-11  09:39:28      -500  2817365839   \n",
       "92024  2020-02-17 07:03:06 EST  2020-02-17  07:03:06      -500  1412497622   \n",
       "\n",
       "            username  \\\n",
       "68188  jarokrolewski   \n",
       "92024    clausematch   \n",
       "\n",
       "                                                                                                                                                         tweet  \\\n",
       "68188                                                                                                               really cool  for wallet management           \n",
       "92024  digital bank  has more than tripled its valuation to  bn   bn after closing the funding round heres how this fast growing fintech ensures compliance      \n",
       "\n",
       "       ...       username  \\\n",
       "68188  ...  jarokrolewski   \n",
       "92024  ...    clausematch   \n",
       "\n",
       "                                                                                                                                                         tweet  \\\n",
       "68188                                                                                                               really cool  for wallet management           \n",
       "92024  digital bank  has more than tripled its valuation to  bn   bn after closing the funding round heres how this fast growing fintech ensures compliance      \n",
       "\n",
       "                                                                                                                                                tweet_compound  \\\n",
       "68188                                                                                                               really cool  for wallet management           \n",
       "92024  digital bank  has more than tripled its valuation to  bn   bn after closing the funding round heres how this fast growing fintech ensures compliance      \n",
       "\n",
       "      topic  neg    neu    pos compound month day  \n",
       "68188     2  0.0  0.607  0.393   0.3804     5  11  \n",
       "92024     2  0.0  0.931  0.069   0.1779     2  17  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with option_context('display.max_colwidth', 600):\n",
    "    display(full_df[(full_df['topic']==2)].sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic  month\n",
       "0      1        0.031812\n",
       "       2        0.021700\n",
       "       3        0.021529\n",
       "       4        0.043448\n",
       "       5        0.041336\n",
       "       6       -0.010325\n",
       "       7        0.026898\n",
       "       8        0.009379\n",
       "       9        0.041506\n",
       "       10       0.043509\n",
       "       11       0.026092\n",
       "       12       0.047148\n",
       "1      1        0.066609\n",
       "       2        0.091436\n",
       "       3        0.102382\n",
       "       4        0.057057\n",
       "       5        0.083351\n",
       "       6        0.056466\n",
       "       7        0.087909\n",
       "       8        0.094616\n",
       "       9        0.087912\n",
       "       10       0.104791\n",
       "       11       0.076863\n",
       "       12       0.082567\n",
       "2      1        0.119911\n",
       "       2        0.197166\n",
       "       3        0.167375\n",
       "       4        0.130164\n",
       "       5        0.110703\n",
       "       6        0.078551\n",
       "       7        0.161052\n",
       "       8        0.112354\n",
       "       9        0.143191\n",
       "       10       0.135144\n",
       "       11       0.043564\n",
       "       12       0.184340\n",
       "3      1       -0.048408\n",
       "       2       -0.012772\n",
       "       3        0.021191\n",
       "       4       -0.034184\n",
       "       5       -0.048439\n",
       "       6       -0.069412\n",
       "       7       -0.019001\n",
       "       8       -0.047790\n",
       "       9       -0.041621\n",
       "       10       0.008162\n",
       "       11      -0.042460\n",
       "       12       0.006841\n",
       "4      1        0.165750\n",
       "       2        0.171461\n",
       "       3        0.173450\n",
       "       4        0.146192\n",
       "       5        0.161352\n",
       "       6        0.128809\n",
       "       7        0.165097\n",
       "       8        0.167948\n",
       "       9        0.156479\n",
       "       10       0.165321\n",
       "       11       0.175600\n",
       "       12       0.161392\n",
       "Name: compound, dtype: float64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby(['topic','month'])['compound'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "0    15340\n",
       "1    22547\n",
       "2    19388\n",
       "3    14005\n",
       "4    28016\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.groupby('topic')['topic'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99296, 34)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export to csv for Tableau analysis\n",
    "#full_df.to_csv('datatableau.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFI-DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display top n terms associated with each topic\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tawneykirkland/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'course', \"n't\", 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99296, 14602)\n"
     ]
    }
   ],
   "source": [
    "# tuning vectorizer params\n",
    "tf_idf = TfidfVectorizer(stop_words=stop,\n",
    "                         tokenizer=word_tokenize,\n",
    "                         min_df= 2,\n",
    "                         max_df= 0.9)\n",
    "\n",
    "# document-term matrix\n",
    "doc_word2 = tf_idf.fit_transform(df_analysis.tweet_compound)\n",
    "print(doc_word2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "account, lock, access, months, reason\n",
      "\n",
      "Topic  1\n",
      "help, account, problem, revolut, trying\n",
      "\n",
      "Topic  2\n",
      "app, access, contact, support, new\n",
      "\n",
      "Topic  3\n",
      "waiting, chat, agent, live, reply\n",
      "\n",
      "Topic  4\n",
      "money, revolut, bank, transfer, send\n"
     ]
    }
   ],
   "source": [
    "n = 5 # number of topics\n",
    "\n",
    "# model selection, fit/trans, and hyperparameter tuning\n",
    "nmf_model_2 = NMF(n_components =n)\n",
    "\n",
    "# doc-topic matrix\n",
    "doc_topic2 = nmf_model_2.fit_transform(doc_word2)\n",
    "\n",
    "# creating ids for each topic\n",
    "topic_ids2 = [\"topic\"+str(val) for val in range(n)]\n",
    "\n",
    "# topic-term matrix\n",
    "topic_word2 = pd.DataFrame(nmf_model_2.components_.round(n),\n",
    "             index = topic_ids2,\n",
    "             columns = tf_idf.get_feature_names())\n",
    "\n",
    "# prints top x words in each topic\n",
    "display_topics(nmf_model_2, \n",
    "               tf_idf.get_feature_names(), \n",
    "               5) # number of top words/topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in merged_df['tweet_compound']:\n",
    "    tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, test = train_test_split(merged_df,train_size = 0.05,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr.to_csv('data_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
